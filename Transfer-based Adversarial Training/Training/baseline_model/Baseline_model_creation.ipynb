{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVyanlHtNq7a"
      },
      "source": [
        "Set as GPU before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyhRlZbbNpc1",
        "outputId": "cd10d85a-1082-4a94-bf71-451fc641075a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUSYMpnQOARa",
        "outputId": "d47ead41-de12-44e6-9b4b-af00888aa990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.12/dist-packages (from face_recognition) (8.3.1)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.12/dist-packages (from face_recognition) (19.24.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from face_recognition) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from face_recognition) (11.3.0)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566166 sha256=2ec4a55cdd7aa4dab681252da0c2b99dfef4b219b9156b9967034444579193b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/47/c8/f44c5aebb7507f7c8a2c0bd23151d732d0f0bd6884ad4ac635\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision timm pandas scikit-learn matplotlib seaborn opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Important imports\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torch import nn\n",
        "from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import cv2\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the model and preparing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Definition\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
        "        super(Model, self).__init__()\n",
        "        base_model = models.resnext50_32x4d(weights=None)\n",
        "        self.cnn = nn.Sequential(*list(base_model.children())[:-2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.cnn(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size, seq_length, 2048)\n",
        "        x_lstm, _ = self.lstm(x)\n",
        "        x = torch.mean(x_lstm, dim=1)\n",
        "        return fmap, self.dp(self.linear(x))\n",
        "\n",
        "# Dataset with padding and skip zero frames\n",
        "class video_dataset(Dataset):\n",
        "    def __init__(self, video_names, labels, sequence_length=20, transform=None):\n",
        "        self.video_names = video_names\n",
        "        self.labels = labels\n",
        "        self.count = sequence_length\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        frames = []\n",
        "        temp_video = os.path.basename(video_path)\n",
        "        label = self.labels.loc[self.labels[\"file\"] == temp_video, \"label\"].values[0]\n",
        "\n",
        "        for frame in self.frame_extract(video_path):\n",
        "            frames.append(self.transform(frame))\n",
        "            if len(frames) == self.count:\n",
        "                break\n",
        "\n",
        "        if len(frames) == 0:\n",
        "            # Skip videos with 0 frames\n",
        "            return self.__getitem__((idx + 1) % len(self.video_names))\n",
        "\n",
        "        frames = torch.stack(frames)\n",
        "\n",
        "        # Pad short videos by repeating last frame\n",
        "        if frames.shape[0] < self.count:\n",
        "            pad_count = self.count - frames.shape[0]\n",
        "            last_frame = frames[-1].unsqueeze(0).repeat(pad_count, 1, 1, 1)\n",
        "            frames = torch.cat([frames, last_frame], dim=0)\n",
        "\n",
        "        return frames, label\n",
        "\n",
        "    @staticmethod\n",
        "    def frame_extract(path):\n",
        "        vidObj = cv2.VideoCapture(path)\n",
        "        success = True\n",
        "        while success:\n",
        "            success, image = vidObj.read()\n",
        "            if success:\n",
        "                yield image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        inputs, targets = inputs.to(device), targets.long().to(device)\n",
        "        _, outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        acc = calculate_accuracy(outputs, targets)\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sys.stdout.write(\n",
        "            f\"\\r[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(data_loader)}] [Train Loss: {losses.avg:.4f}, Train Acc: {accuracies.avg:.2f}%]\"\n",
        "        )\n",
        "    print()\n",
        "    return losses.avg, accuracies.avg\n",
        "\n",
        "def test(epoch, model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    pred, true, probs_list = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            inputs, targets = inputs.to(device), targets.long().to(device)\n",
        "            _, outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "            loss = criterion(outputs, targets)\n",
        "            acc = calculate_accuracy(outputs, targets)\n",
        "            _, p = torch.max(outputs, 1)\n",
        "            true += targets.cpu().numpy().tolist()\n",
        "            pred += p.cpu().numpy().tolist()\n",
        "            probs_list.extend(probs.tolist())\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            accuracies.update(acc, inputs.size(0))\n",
        "    return true, pred, probs_list, losses.avg, accuracies.avg\n",
        "\n",
        "\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, pred = outputs.max(1)\n",
        "    correct = pred.eq(targets).sum().item()\n",
        "    return 100 * correct / targets.size(0)\n",
        "\n",
        "\n",
        "class AverageMeter():\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = self.avg = self.sum = self.count = 0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    torch.save(state, filename)\n",
        "    print(f\"\\nCheckpoint saved: {filename}\")\n",
        "\n",
        "\n",
        "def read_list(txt_path):\n",
        "    with open(txt_path, 'r') as f:\n",
        "        return [line.strip() for line in f.readlines()]\n",
        "\n",
        "\n",
        "def assign_label(path):\n",
        "    path_low = path.lower()\n",
        "    if \"fake\" in path_low or \"deepfake\" in path_low or \"manipulated\" in path_low:\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Begin training here. (Can perform early stopping if you notice the validation accuracy decreasing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kkmfp08BN2Q9",
        "outputId": "5f1afbcb-b2f6-4e23-9a85-59b13b288117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0/50] [Batch 244/245] [Train Loss: 0.7116, Train Acc: 52.24%]\n",
            "Epoch 0 Validation Accuracy: 53.93%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_0.pt\n",
            "[Epoch 1/50] [Batch 244/245] [Train Loss: 0.6829, Train Acc: 57.45%]\n",
            "Epoch 1 Validation Accuracy: 61.26%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_1.pt\n",
            "[Epoch 2/50] [Batch 244/245] [Train Loss: 0.7042, Train Acc: 50.61%]\n",
            "Epoch 2 Validation Accuracy: 63.35%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_2.pt\n",
            "[Epoch 3/50] [Batch 244/245] [Train Loss: 0.6870, Train Acc: 54.39%]\n",
            "Epoch 3 Validation Accuracy: 66.49%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_3.pt\n",
            "[Epoch 4/50] [Batch 244/245] [Train Loss: 0.6861, Train Acc: 55.71%]\n",
            "Epoch 4 Validation Accuracy: 65.97%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_4.pt\n",
            "[Epoch 5/50] [Batch 244/245] [Train Loss: 0.6728, Train Acc: 57.14%]\n",
            "Epoch 5 Validation Accuracy: 70.16%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_5.pt\n",
            "[Epoch 6/50] [Batch 244/245] [Train Loss: 0.6615, Train Acc: 58.98%]\n",
            "Epoch 6 Validation Accuracy: 67.54%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_6.pt\n",
            "[Epoch 7/50] [Batch 244/245] [Train Loss: 0.6592, Train Acc: 62.96%]\n",
            "Epoch 7 Validation Accuracy: 66.49%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_7.pt\n",
            "[Epoch 8/50] [Batch 244/245] [Train Loss: 0.6454, Train Acc: 63.67%]\n",
            "Epoch 8 Validation Accuracy: 67.54%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_8.pt\n",
            "[Epoch 9/50] [Batch 244/245] [Train Loss: 0.6323, Train Acc: 64.29%]\n",
            "Epoch 9 Validation Accuracy: 58.64%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_9.pt\n",
            "[Epoch 10/50] [Batch 244/245] [Train Loss: 0.6249, Train Acc: 63.27%]\n",
            "Epoch 10 Validation Accuracy: 64.92%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_10.pt\n",
            "[Epoch 11/50] [Batch 244/245] [Train Loss: 0.6087, Train Acc: 67.86%]\n",
            "Epoch 11 Validation Accuracy: 62.30%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_11.pt\n",
            "[Epoch 12/50] [Batch 244/245] [Train Loss: 0.6023, Train Acc: 66.53%]\n",
            "Epoch 12 Validation Accuracy: 65.45%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_12.pt\n",
            "[Epoch 13/50] [Batch 244/245] [Train Loss: 0.5911, Train Acc: 69.69%]\n",
            "Epoch 13 Validation Accuracy: 66.49%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_13.pt\n",
            "[Epoch 14/50] [Batch 244/245] [Train Loss: 0.5672, Train Acc: 70.31%]\n",
            "Epoch 14 Validation Accuracy: 70.16%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_14.pt\n",
            "[Epoch 15/50] [Batch 244/245] [Train Loss: 0.5488, Train Acc: 72.55%]\n",
            "Epoch 15 Validation Accuracy: 71.73%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_15.pt\n",
            "[Epoch 16/50] [Batch 244/245] [Train Loss: 0.5903, Train Acc: 68.88%]\n",
            "Epoch 16 Validation Accuracy: 59.16%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_16.pt\n",
            "[Epoch 17/50] [Batch 244/245] [Train Loss: 0.5859, Train Acc: 68.98%]\n",
            "Epoch 17 Validation Accuracy: 69.63%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_17.pt\n",
            "[Epoch 18/50] [Batch 244/245] [Train Loss: 0.5430, Train Acc: 71.12%]\n",
            "Epoch 18 Validation Accuracy: 67.02%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_18.pt\n",
            "[Epoch 19/50] [Batch 244/245] [Train Loss: 0.5440, Train Acc: 73.78%]\n",
            "Epoch 19 Validation Accuracy: 61.78%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_19.pt\n",
            "[Epoch 20/50] [Batch 244/245] [Train Loss: 0.5458, Train Acc: 72.86%]\n",
            "Epoch 20 Validation Accuracy: 61.26%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_20.pt\n",
            "[Epoch 21/50] [Batch 244/245] [Train Loss: 0.5404, Train Acc: 73.06%]\n",
            "Epoch 21 Validation Accuracy: 59.16%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_21.pt\n",
            "[Epoch 22/50] [Batch 244/245] [Train Loss: 0.5345, Train Acc: 73.57%]\n",
            "Epoch 22 Validation Accuracy: 62.30%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_22.pt\n",
            "[Epoch 23/50] [Batch 244/245] [Train Loss: 0.5230, Train Acc: 73.47%]\n",
            "Epoch 23 Validation Accuracy: 68.06%\n",
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_23.pt\n",
            "[Epoch 24/50] [Batch 15/245] [Train Loss: 0.4036, Train Acc: 81.25%]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3271216700.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0mtrain_loss_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3271216700.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, num_epochs, data_loader, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3271216700.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input type {npimg.dtype} is not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_Image_fromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36m_Image_fromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to convert obj into contiguous format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontiguous_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3150\u001b[0m     \u001b[0m_check_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3152\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m         \u001b[0mdecoder_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   3115\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_ints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "SEQ_LEN = 20\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 50 # update to the max epoch you want\n",
        "LR = 1e-5\n",
        "IM_SIZE = 112\n",
        "\n",
        "# Dataset splits\n",
        "train_files = read_list(\"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/baseline_splits/train.txt\")\n",
        "val_files = read_list(\"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/baseline_splits/val.txt\")\n",
        "\n",
        "\n",
        "train_labels = pd.DataFrame({\"file\":[os.path.basename(p) for p in train_files],\n",
        "                             \"label\":[assign_label(p) for p in train_files]})\n",
        "val_labels = pd.DataFrame({\"file\":[os.path.basename(p) for p in val_files],\n",
        "                           \"label\":[assign_label(p) for p in val_files]})\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                      transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "                                      transforms.ToTensor()])\n",
        "val_transform = train_transform\n",
        "\n",
        "# Dataset and loaders\n",
        "train_dataset = video_dataset(train_files, train_labels, SEQ_LEN, transform=train_transform)\n",
        "val_dataset   = video_dataset(val_files, val_labels, SEQ_LEN, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# Model, optimizer, criterion\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model(2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Uncomment the following code if we want to load from a checkpoint\n",
        "\n",
        "# checkpoint_path = \"/content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_39.pt\"\n",
        "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "# model.load_state_dict(checkpoint[\"model_state\"])\n",
        "# optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "# start_epoch = checkpoint[\"epoch\"] + 1  # continue from next epoch\n",
        "\n",
        "# print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "\n",
        "# Training loop\n",
        "train_loss_avg, train_acc_list = [], []\n",
        "val_loss_avg, val_acc_list = [], []\n",
        "\n",
        "# Change this to the path you want to save your validation accuracy\n",
        "val_acc_file = \"/content/drive/MyDrive/deepfake_detection_project/Baseline_model/valid_accs/val_accuracy.txt\" \n",
        "\n",
        "for epoch in range(0, NUM_EPOCHS + 1): # If we start from an epoch change 0 to the start_epoch\n",
        "    train_loss, train_acc = train_epoch(epoch, NUM_EPOCHS, train_loader, model, criterion, optimizer)\n",
        "    train_loss_avg.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    y_true, y_pred, y_probs, val_loss, val_acc = test(epoch, model, val_loader, criterion)\n",
        "    val_loss_avg.append(val_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    # Print and save validation accuracy\n",
        "    print(f\"Epoch {epoch} Validation Accuracy: {val_acc:.2f}%\")\n",
        "    with open(val_acc_file, \"a\") as f:\n",
        "        f.write(f\"Epoch {epoch}: {val_acc:.2f}%\\n\")\n",
        "\n",
        "    # Save model checkpoint\n",
        "    save_checkpoint({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_acc\": val_acc\n",
        "    }, filename=f\"/content/drive/MyDrive/deepfake_detection_project/Baseline_model/new_models/model_epoch_{epoch}.pt\") \n",
        "    # Change this to the path you want to save the model checkpoint at each epoch\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
